dataset: demo_dataset       # Provide dataset name to be used. Dataset name provided through CLI will get priority

generate_ground_truth: true    # Provide 'true' if groundtruth needs to be generated with an LLM, else provide 'false' and provide absolute path to groundtruth file
extract_ground_truth: false
ground_truth: ""               # Provide absolute path to groundtruth file if 'generate_ground_truth' is set to 'false' and 'extract_ground_truth' is set to 'true'

# Provide LLM details to generate questions
questions_generator:
   prompt: "default"
   num_questions_per_chunk: 1
   max_chunks: 10       # Maximum number of chunks to be used for question generation
   llm: dkubex          # OPTIONS: dkubex/openai
   llm_url: "http://xxx.xxx.xxx.xxx:xxxxx/v1/"     # Provide service endpoint of LLM deployment in DKubeX. If using OpenAI, keep blank
   llmkey: eyJhbG*********************r5mJY                        # Provide serving token of LLM deployment in DKubeX. If using OpenAI, provide OpenAI API key
   max_tokens: 2048

rag_configuration: <path to RAG config>         # Provide absolute path to RAG config file

vectorstore: weaviate_vectorstore
weaviate_vectorstore:
   url: ""
   auth_key: ""
   provider: dkubex
   properties:
   - paperdocs
   - dkubexfm

evaluator:
 - semantic_similarity_evaluator       # Vector Similarity
 - correctness_evaluator               # LLM Similarity
#  - answer_relevancy_evaluator
#  - retrieval_evaluator

semantic_similarity_evaluator:
   prompt: "default"
   embedding_provider: sky    # OPTIONS: dkubex, sky
   embedding_url: "http://xxx.xxx.xxx.xxx:xxxxx/v1/"           # Provide service endpoint of local/skypilot embedding model deployment in DKubeX
   embedding_key: eyJhbG**********************DJ26vc            # Provide serving token of local/skypilot embeddding model deployment in DKubeX

correctness_evaluator:
   # Prompt to be used in the evaluation process
   prompt: |
       You are an expert evaluation system for a question answering chatbot.

       You are given the following information:
       - a user query, and
       - a generated answer

       You may also be given a reference answer to use for reference in your evaluation.

       Your job is to judge the relevance and correctness of the generated answer.
       Output a single score that represents a holistic evaluation.
       You must return your response in a line with only the score.
       Do not return answers in any other format.
       On a separate line provide your reasoning for the score as well.

       Please act as an impartial judge and evaluate the similarity of the response provided by
       an assistant against the reference answer for user question below.
       You will be given a reference answer and the assistant's answer.
       Your job is to compare the assistant's answer with the reference answer for similarity
       in the context of the user's question.
       Provide a score between 1 and 10 with an interval of 1
       Do not allow the length of the response to influence your evaluation.
   llm: dkubex  # OPTIONS: dkubex/openai
   llm_url: "http://xxx.xxx.xxx.xxx:xxxxx/v1/"     # Provide service endpoint of LLM deployment in DKubeX. If using OpenAI, keep blank
   llmkey: eyJhbG*********************r5mJY                        # Provide serving token of LLM deployment in DKubeX. If using OpenAI, provide OpenAI API key
   max_tokens: 2048
   output_parser: fetch_llm_score

# cross_vector_similarity_evaluator:
#     prompt: "default"
#     llm: openai         # dkubex
#     llmkey: "sk-4aY***********************OZRLQe" # <llmkey>
#     llm_url: ""             # http://xxx.xxx.xxx.xxx/v1/
#     max_tokens: 2048
#     rag_configuration: "/home/demo/simple_rag.yaml"

# answer_relevancy_evaluator:
#    prompt: "default"
#    llm: dkubex      # OPTIONS: dkubex/openai
#    llm_url: "http://xxx.xxx.xxx.xxx:xxxxx/v1/"     # Provide service endpoint of LLM deployment in DKubeX. If using OpenAI, keep blank
#    llmkey: eyJhbG*********************r5mJY                        # Provide serving token of LLM deployment in DKubeX. If using OpenAI, provide OpenAI API key
#    max_tokens: 2048

# retrieval_evaluator:
#    weaviate_vectorstore:
#        kind: weaviate
#        vectorstore_provider: dkubex
#        textkey: paperdocs
#        # Choose embedding model type to be used. Once done, provide details in the appropriate section below.
#        embedding_provider: "sky"    # OPTIONS: dkubex, sky, huggingface, openai
#        embedding_url: "http://xxx.xxx.xxx.xxx:xxxxx/v1/"            # Provide service endpoint of local/skypilot embedding model deployment in DKubeX
#        embedding_key: eyJhbG*********************r5mJY                         # Provide serving token of local/skypilot embeddding model deployment in DKubeX
#        embedding_model: "BAAI/bge-large-en-v1.5"                               # Embedding model name
#        llmkey: ""                                                              # Provide OpenAI API key if using OpenAI, else keep blank
#        similarity_top_k: 3
#    metrics:
#    - mrr
#    - hit_rate

# Provide MLFlow Experiment Name
tracking:
   experiment: demo-eval

